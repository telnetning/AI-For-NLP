{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"/Users/telnetning/Downloads/data/zhwiki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 暂时使用整个 AASIMP（AA转换成简体后的文件目录） 目录下的 100 个文件: wiki_00 ~ wiki_99\n",
    "import os\n",
    "# for file in os.listdir(DATA_PATH + \"AASIMP\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-11_00-31-47_26801/logs.\n",
      "Waiting for redis server at 127.0.0.1:11504 to respond...\n",
      "Waiting for redis server at 127.0.0.1:48807 to respond...\n",
      "Starting Redis shard with 10.0 GB max memory.\n",
      "Starting the Plasma object store with 5.0 GB memory using /tmp.\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [file for file in os.listdir(DATA_PATH + \"AASIMP\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki_73', 'wiki_87', 'wiki_80', 'wiki_74', 'wiki_89', 'wiki_42', 'wiki_45', 'wiki_11', 'wiki_16', 'wiki_29', 'wiki_20', 'wiki_27', 'wiki_18', 'wiki_44', 'wiki_88', 'wiki_43', 'wiki_75', 'wiki_81', 'wiki_86', 'wiki_72', 'wiki_26', 'wiki_19', 'wiki_21', 'wiki_17', 'wiki_28', 'wiki_10', 'wiki_32', 'wiki_35', 'wiki_03', 'wiki_04', 'wiki_50', 'wiki_57', 'wiki_68', 'wiki_61', 'wiki_95', 'wiki_92', 'wiki_66', 'wiki_59', 'wiki_05', 'wiki_02', 'wiki_34', 'wiki_33', 'wiki_67', 'wiki_93', 'wiki_58', 'wiki_94', 'wiki_60', 'wiki_56', 'wiki_69', 'wiki_51', 'wiki_15', 'wiki_12', 'wiki_24', 'wiki_23', 'wiki_48', 'wiki_83', 'wiki_77', 'wiki_70', 'wiki_84', 'wiki_79', 'wiki_46', 'wiki_41', 'wiki_22', 'wiki_25', 'wiki_13', 'wiki_14', 'wiki_40', 'wiki_78', 'wiki_47', 'wiki_85', 'wiki_71', 'wiki_49', 'wiki_76', 'wiki_82', 'wiki_54', 'wiki_53', 'wiki_98', 'wiki_91', 'wiki_65', 'wiki_62', 'wiki_96', 'wiki_09', 'wiki_36', 'wiki_31', 'wiki_38', 'wiki_07', 'wiki_00', 'wiki_97', 'wiki_63', 'wiki_64', 'wiki_90', 'wiki_52', 'wiki_99', 'wiki_55', 'wiki_01', 'wiki_39', 'wiki_06', 'wiki_30', 'wiki_08', 'wiki_37']\n"
     ]
    }
   ],
   "source": [
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文章进行处理的函数\n",
    "import re\n",
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "for file in all_files:\n",
    "    with open(DATA_PATH + \"AASIMP/\" + file) as f:\n",
    "        data += token(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37162299\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8b/_681s2t12zzfg84s_yp6lfm00000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/8b/_681s2t12zzfg84s_yp6lfm00000gn/T/jieba.cache\n",
      "Loading model cost 1.174 seconds.\n",
      "Loading model cost 1.174 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "ALL_TOKNES = cut(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20965362\n"
     ]
    }
   ],
   "source": [
    "print(len(ALL_TOKNES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_token = [t for t in ALL_TOKNES if t.strip() and t != 'n'] # 去除空白字符和换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17220910"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc',\n",
       " 'id',\n",
       " '47081',\n",
       " 'url',\n",
       " 'https',\n",
       " 'zh',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'curid',\n",
       " '47081',\n",
       " 'title',\n",
       " '公有',\n",
       " '领域',\n",
       " '公有',\n",
       " '领域',\n",
       " '公有',\n",
       " '领域',\n",
       " '是',\n",
       " '人类']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words_count = Counter(valid_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 984867),\n",
       " ('在', 255319),\n",
       " ('年', 207999),\n",
       " ('是', 182701),\n",
       " ('和', 157811),\n",
       " ('了', 125518),\n",
       " ('为', 115956),\n",
       " ('与', 91380),\n",
       " ('有', 87706),\n",
       " ('月', 86406)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common 返回前 x 个频次最高的，返回类型 [(word1, frequency1), (word2, frequency2)...]\n",
    "all_frequences = [f for w, f in words_count.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences_sum = sum(all_frequences)\n",
    "# 某个单词出现的概率 = 该单词出现的次数 / 所有单词一共出现的次数\n",
    "def get_prob(word):\n",
    "    return words_count[word] / frequences_sum if word in words_count else 1 / frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00030753310945821097"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob('我们')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004489890487784908"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob('存在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00019424060633265025"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_prob('数学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.297101604967449e-06"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob('火锅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 以上可以基本看出，词频和日常生活的词频可能不太一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)\n",
    "\n",
    "def language_model_one_gram(sentence):\n",
    "    return product(get_prob(word) for word in cut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.896419478569954e-18"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_one_gram('今天我不想上班')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7987803611338926e-15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_one_gram('明天天气不错')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.194485194474578e-14"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_one_gram('这是一个数学问题')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件概率，A事件出现的情况下B事件出现的概率 = (AB事件出现的概率) / A事件出现的概率\n",
    "all_2_grams_words = [''.join(valid_token[i:i+2]) for i in range(len(valid_token[:-2]))] # AB连着出现\n",
    "\n",
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    # 联合概率\n",
    "    if w1 + w2 in all_2_grams_words:\n",
    "         return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docid',\n",
       " 'id47081',\n",
       " '47081url',\n",
       " 'urlhttps',\n",
       " 'httpszh',\n",
       " 'zhwikipedia',\n",
       " 'wikipediaorg',\n",
       " 'orgwiki',\n",
       " 'wikicurid',\n",
       " 'curid47081',\n",
       " '47081title',\n",
       " 'title公有',\n",
       " '公有领域',\n",
       " '领域公有',\n",
       " '公有领域',\n",
       " '领域公有',\n",
       " '公有领域',\n",
       " '领域是',\n",
       " '是人类',\n",
       " '人类的']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_2_grams_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.529377893430474e-06"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combination_prob('公有','领域')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.806894735167275e-08"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combination_prob('回','深圳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.806894735167275e-08"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combination_prob('世界','大战')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word) \n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob # 概率相乘\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1125386358018594e-11"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('明天要加班')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8155058338495557e-12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('明天不加班')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.23517493404459e-10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('小猫吃鱼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7057499267162104e-11"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('鱼吃老虎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7057499267162104e-11"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('鱼吃小猫')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4306767116841882e-10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('数学是一门学科')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.806894735167275e-08"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_of_2_gram('宇宙很大')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据量太小，合理性有限，并且数据太靠近科学，而远离生活，场景有限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+  数据量不仅要大，而且要来源广泛，才可能更准确\n",
    "+  如果可以区分一个问题的场景，采用对应场景的数据集，可能得到的结果会更准确\n",
    "+  工程处理上，大量数据，可以采用线下分词，尤其对于 1-Gram，可以线下分词存到数据库，线上直接访问数据库查频次\n",
    "+  modin 比 pandas 更快，简繁转换，出了 linux 自带 opencc，还有 hanziconv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
